{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir=f'{Path(os.getcwd()).parent}'\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "from libs.vad import VAD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "def readAudioFile(audio_path):\n",
    "    audio, sample_rate = sf.read(audio_path)\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio[:, 0]\n",
    "    if sample_rate != 16000:\n",
    "        audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)\n",
    "    return audio.astype(\"float16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "whole_audio = np.concatenate([readAudioFile(\"/mnt/e/dataset/jsut_ver1.1/loanword128/wav/LOANWORD128_064.wav\"),\n",
    "                              np.zeros(3200), \n",
    "                              readAudioFile(\"/mnt/e/dataset/jsut_ver1.1/loanword128/wav/LOANWORD128_065.wav\")])\n",
    "\n",
    "# 40msecで分割する\n",
    "window_size = int(16000 * 0.04) # 320\n",
    "split_audio = [whole_audio[i:i+window_size] for i in range(0, len(whole_audio), window_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sylx/project/altbot/ai-service/.model_cache/hub/snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.hub.set_dir(f'{base_dir}/.model_cache/hub',)\n",
    "vad_model, _ = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=False,\n",
    "                              onnx=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sylx/project/altbot/ai-service/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at AndrewMcDowell/wav2vec2-xls-r-300m-japanese were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at AndrewMcDowell/wav2vec2-xls-r-300m-japanese and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=181, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoFeatureExtractor,Wav2Vec2ForCTC,Wav2Vec2Processor\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"AndrewMcDowell/wav2vec2-xls-r-300m-japanese\"\n",
    "#MODEL_NAME = \"facebook/wav2vec2-lv-60-espeak-cv-ft\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME,torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "processor=Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ケェヴェリンとうのたたかいとは、にっほんぐんのまもるクェヴェリンかんしょうへアメリカぐんがしんこうしたおかなわれたせんとうである、、コンサーウンはかんこくのはいゆうである。'],\n",
       " torch.Size([1, 512, 181])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 40msecで分割した場合とそうでない場合とで、認識結果はどう変化するか？\n",
    "\n",
    "input_values = feature_extractor(whole_audio, sampling_rate=16000,return_tensors=\"pt\").input_values.to(\"cuda:0\")\n",
    "# to float16\n",
    "input_values=input_values.half()\n",
    "logits = model(input_values)[0]\n",
    "pred_ids = torch.argmax(logits, axis=-1)\n",
    "text=processor.batch_decode(pred_ids)\n",
    "\n",
    "[text,logits.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6//5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同一区間は与えられたキーワードのどれかであるか、どれでもないか、なので、複数キーワードを一括で処理できるようにする\n",
    "\n",
    "class Beam:\n",
    "    def __init__(self,word,priority,result):\n",
    "        self.id=word\n",
    "        self.priority=priority\n",
    "        self.result=result\n",
    "        self.ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
    "        self.start_index=0\n",
    "        self.prob_total=0.0\n",
    "        self.unmatch_count=0\n",
    "        # 許容するunmatchの回数(6文字ごとに1回) 4文字未満なら-1(一度もunmatchできない)\n",
    "        self.unmatch_threshold=len(self.ids)//4 - 1\n",
    "        self.id_index=0\n",
    "        self.forked=False\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.id} {self.result}\"\n",
    "    \n",
    "    def reset(self):\n",
    "        self.start_index=0\n",
    "        self.prob_total=0.0\n",
    "        self.unmatch_count=0\n",
    "        self.id_index=0\n",
    "\n",
    "    # 不一致の時のハンドラ。余計な発音が挟まってる場合があるので、二回連続まで許容する\n",
    "    def unmatch(self):\n",
    "        #print(f\"unmatch {i} expect {tokenizer.convert_ids_to_tokens(ids[id_index])}({ids[id_index]}) top1 is {tokenizer.convert_ids_to_tokens(top1_id)}({top1_id}) current={current_prob} last={last_id_prob} pad={pad_prob} \")\n",
    "        if self.unmatch_count > self.unmatch_threshold:\n",
    "            #print(f\"{self.id} unmatch count is {self.unmatch_count} so reset\")\n",
    "            self.reset()\n",
    "            return\n",
    "        else:\n",
    "            # 余計な発音の場合と、完全に聞き取れなかった場合がある。後者の場合は、id_index++しないといけないので分岐させる\n",
    "            forkBeam=None\n",
    "            if self.id_index > 0 and self.id_index+1 < len(self.ids) and self.forked is False:\n",
    "                forkBeam=Beam(self.id,self.priority,self.result)\n",
    "                forkBeam.id_index=self.id_index+1\n",
    "                forkBeam.prob_total=self.prob_total\n",
    "                forkBeam.start_index=self.start_index\n",
    "                forkBeam.forked=True\n",
    "            self.unmatch_count+=1\n",
    "            return forkBeam\n",
    "        \n",
    "    def step(self,probs,step,threshold):        \n",
    "        current_id=self.ids[self.id_index]\n",
    "        step_prob=probs[current_id]\n",
    "        #last_id_prob=probs[self.ids[self.id_index-1]] if self.id_index > 0 else 0.0\n",
    "\n",
    "        # id_index==0の時は、BEAM段階で刈り取られているので、調べるまでもなく、thresholdを上回っている（筈）\n",
    "        if self.id_index == 0:\n",
    "            self.start_index=step\n",
    "        elif step_prob < threshold:\n",
    "            return self.unmatch()\n",
    "\n",
    "        self.id_index+=1\n",
    "        self.unmatch_count=0\n",
    "        self.prob_total+=step_prob\n",
    "        if self.id_index == len(self.ids):\n",
    "            #終了\n",
    "            # 0.01以下の場合は、結果としては切り捨てる\n",
    "            result_prob= self.current_prob().item()\n",
    "            if result_prob > 0.01:\n",
    "                self.result.append({\n",
    "                    \"word\": self.id,\n",
    "                    \"start\": self.start_index,\n",
    "                    \"end\": step,\n",
    "                    \"prob\": result_prob\n",
    "                })\n",
    "            self.reset()\n",
    "            return\n",
    "\n",
    "    def current_prob(self):\n",
    "        if self.id_index > 0:\n",
    "            return self.prob_total / self.id_index\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def max_prob(self):\n",
    "        return max(self.result,key=lambda x:x[\"prob\"])\n",
    "\n",
    "def get_keywords_avgprobs(words,logits,num_beams=5,topk=5,without_pad=True):\n",
    "    beams={}\n",
    "    result=[]\n",
    "    for i,word in enumerate(words):\n",
    "        beam = Beam(word,i,result)\n",
    "        first_token=beam.ids[0]\n",
    "        if beams.get(first_token) is None:\n",
    "            beams[first_token]=[]\n",
    "        beams[first_token].append(beam)\n",
    "\n",
    "    logits_prob=torch.softmax(logits[0,:,:],dim=1, dtype=torch.float16)\n",
    "    # 上位N件の確率を取得\n",
    "    topk_values, topk_indices = torch.topk(logits_prob, topk)\n",
    "    threshold = topk_values[:,-1]\n",
    "\n",
    "    # padが一位のstepを除外しておく（10倍以上高速化するが認識率は悪化する) \n",
    "    if without_pad:\n",
    "        steps=torch.where(topk_indices[:,0]!=tokenizer.pad_token_id)[0].tolist()\n",
    "    else:\n",
    "        steps=range(len(logits_prob))\n",
    "\n",
    "    current_beams=[]\n",
    "    for i in steps:\n",
    "        topKList=topk_indices[i,:].tolist()\n",
    "        # topKにあるものを候補に加える(末尾に)これはnum_beamsとは別枠\n",
    "        for id in topKList:\n",
    "            current_beams.extend(beams.get(id,[]))\n",
    "        #重複を排除(順序を維持する)\n",
    "        current_beams=sorted(set(current_beams),key=lambda x: -x.id_index)\n",
    "        #print(f\"step={i} {list(map(lambda x:[x.id,x.id_index],current_beams))} {tokenizer.convert_ids_to_tokens(topKList)}\")\n",
    "\n",
    "        next_beams=[]\n",
    "        for beam in current_beams:\n",
    "            forked=beam.step(logits_prob[i,:],i,threshold[i])\n",
    "            if forked is not None:\n",
    "                next_beams.append(forked)\n",
    "            if beam.id_index != 0:\n",
    "                #マッチしたものだけ次も継続する\n",
    "                next_beams.append(beam)\n",
    "            else:\n",
    "                #マッチしなかったものは終了\n",
    "                #print(f\"{beam.id} is unmatch\")\n",
    "                pass\n",
    "\n",
    "        if len(next_beams) > num_beams:\n",
    "            #確率の高いものからnum_beams件だけ残す\n",
    "            current_beams=sorted(next_beams,key=lambda x: -x.current_prob())\n",
    "            #print(f\"cut {len(next_beams)} to {num_beams} -> {list(map(lambda x:[x.id,x.id_index],current_beams[num_beams:]))}\")\n",
    "            for b in current_beams[num_beams:]:\n",
    "                b.reset()\n",
    "            current_beams=current_beams[:num_beams]\n",
    "        else:\n",
    "            current_beams=next_beams\n",
    "\n",
    "    return sorted(result,key=lambda x: -x[\"prob\"])\n",
    "\n",
    "keywords=[\n",
    "        # 適当なユニーク文字列（かな、カタカナ）\n",
    "        \"アメリカ\",\"にっぽん\",\"まもる\",\"クェゼリン\",\"テスト\",\n",
    "        \"あした\",\"あさって\",\"クォン・サンウ\",\"はいゆう\",\"イエス\",\"ノー\",\n",
    "        \"ホグワーツ\",\"ハリーポッター\",\"ハーマイオニー\",\"ロン\",\"ダンブルドア\",\n",
    "        \"まどか\",\"さやか\",\"ほむら\",\"マミ\",\"キュゥべえ\",\n",
    "        \"あおい\",\"あかね\",\"さくら\",\"みどり\",\"きいろ\",\n",
    "        \"ハンバーグ\",\"ステーキ\",\"ミートソース\",\"ミートボール\",\"ハム\",\n",
    "        \"せかい\",\"ちきゅう\",\"うみ\",\"そら\",\"ほし\",\n",
    "        \"くるま\",\"でんしゃ\",\"ひこうき\",\"ふね\",\"じてんしゃ\",\n",
    "        \"ねこ\",\"いぬ\",\"うさぎ\",\"ねずみ\",\"とら\",\n",
    "        \"にんげん\",\"おとこ\",\"おんな\",\"こども\",\"おじいさん\",\n",
    "        \"ブッシュ\",\"トランプ\",\"バイデン\",\"ヒラリー\",\"オバマ\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : 171\n",
      "after : 139\n",
      "0.0s ケヴリント []\n",
      "6.0s セヴェリントうのたたかい []\n",
      "12.0s のたたかいとは、にっぽん [['にっぽん', 0.85107421875]]\n",
      "18.0s はにっほんぐんのまもる [['まもる', 0.87744140625], ['にっぽん', 0.791015625]]\n",
      "24.0s ぐんのまもるクェゼリン [['まもる', 0.91064453125], ['クェゼリン', 0.82275390625]]\n",
      "30.0s クベリンかんしょうえ []\n",
      "36.0s かんしょうへアメリカぐん [['アメリカ', 0.9599609375]]\n",
      "42.0s アメリカぐんがしんこう [['アメリカ', 0.96875]]\n",
      "48.0s なしんこうしたおかなわれ []\n",
      "54.0s したおかなわれたせんとう []\n",
      "60.0s れたせんとうであるほん []\n",
      "66.0s であるホンサーンンはか []\n",
      "72.0s サウンンはかんこくのはい []\n",
      "78.0s こくのはいゆうである [['はいゆう', 0.98095703125]]\n"
     ]
    }
   ],
   "source": [
    "# 60msecで分割する\n",
    "window_size = int(16000 * 0.06) # 320\n",
    "split_audio = [whole_audio[i:i+window_size] for i in range(0, len(whole_audio), window_size)]\n",
    "print(f\"before : {len(split_audio)}\")\n",
    "# vadで無音区間を除去する\n",
    "threshhold=0.2\n",
    "split_audio = [audio for audio in split_audio if vad_model(torch.from_numpy(audio).float(), 16000) > threshhold]\n",
    "print(f\"after : {len(split_audio)}\")\n",
    "chunk_size = 10\n",
    "stride=10\n",
    "for i in range(0, len(split_audio), chunk_size):\n",
    "    # list to flatten\n",
    "    left = i - stride\n",
    "    right = i + chunk_size\n",
    "    if left < 0:\n",
    "        left = 0\n",
    "    if right > len(split_audio):\n",
    "        right = len(split_audio)        \n",
    "    current_audio=np.concatenate(split_audio[left:right])\n",
    "    input_values = feature_extractor(current_audio, sampling_rate=16000,return_tensors=\"pt\").input_values\n",
    "    # to float16\n",
    "    input_values=input_values.half().to(\"cuda:0\")\n",
    "    logits = model(input_values)[0]\n",
    "    probs=get_keywords_avgprobs(keywords,logits,num_beams=5,topk=5,without_pad=True)\n",
    "    pred_ids = torch.argmax(logits, axis=-1)\n",
    "    text=processor.batch_decode(pred_ids)\n",
    "    print(f\"{i*0.06}s {text[0]} {[[p['word'],p['prob']] for p in probs]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
