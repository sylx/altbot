{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vec2でキーワードスポッティング(?)する\n",
    "\n",
    "キーワードスポッティングの要件\n",
    "\n",
    "1. キーワードは事前に登録される（複数）\n",
    "1. 多少の認識エラーを許容し、できれば、認識の正確さを数値化したい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sylx/project/altbot/ai-service/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at AndrewMcDowell/wav2vec2-xls-r-300m-japanese were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at AndrewMcDowell/wav2vec2-xls-r-300m-japanese and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=181, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoFeatureExtractor,Wav2Vec2ForCTC,Wav2Vec2Processor\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"AndrewMcDowell/wav2vec2-xls-r-300m-japanese\"\n",
    "#MODEL_NAME = \"facebook/wav2vec2-lv-60-espeak-cv-ft\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME,torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "processor=Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "def readAudioFile(audio_path):\n",
    "    audio, sample_rate = sf.read(audio_path)\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio[:, 0]\n",
    "    if sample_rate != 16000:\n",
    "        audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)\n",
    "    return audio.astype(\"float16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 001-10.wavで試して時間を計測する\n",
    "import time\n",
    "start_time = time.time()\n",
    "# LOANWORD128_060:フランスの作曲家ギィ・ロパルツが作曲した。\n",
    "# LOANWORD128_061:ギェンツェン・ノルブは、中華人民共和国、チベット自治区出身の、チベット族の僧である。\n",
    "# LOANWORD128_062:どれがクァディ族のものか定かではない。\n",
    "# LOANWORD128_063:クィーンアリア国際空港は、ヨルダンの首都アンマンにある国際空港である。\n",
    "# LOANWORD128_064:クェゼリン島の戦いとは，日本軍の守るクェゼリン環礁へ、アメリカ軍が侵攻して行われた戦闘である。\n",
    "# LOANWORD128_065:クォン・サンウは、韓国の俳優である。\n",
    "# LOANWORD128_066:グァルネリは、イタリア出身の弦楽器製作者一族、または、彼らが制作した弦楽器である。\n",
    "# LOANWORD128_067:グィネヴィアは小惑星帯の外縁部付近に位置する小惑星である。\n",
    "# LOANWORD128_068:親友となったザフトの英雄グゥド・ヴェイアとの戦闘で損傷した。\n",
    "# LOANWORD128_069:グィネヴィアは、伝説的な人物で、アーサー王の王妃。\n",
    "# LOANWORD128_070:グォ・ヨウは，中国北京出身の男性俳優。\n",
    "wavs=range(60,71)\n",
    "for i in wavs:\n",
    "    audio = readAudioFile(f\"/mnt/e/dataset/jsut_ver1.1/loanword128/wav/LOANWORD128_{i:03d}.wav\")\n",
    "    input_values = feature_extractor(audio, sampling_rate=16000,return_tensors=\"pt\").input_values.to(\"cuda:0\")\n",
    "    logits = model(input_values)[0]\n",
    "    pred_ids = torch.argmax(logits, axis=-1)\n",
    "    text=processor.batch_decode(pred_ids)\n",
    "    print(f\"{i:04d}: {text[0]}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "avg_time = elapsed_time / len(wavs)\n",
    "print(f\"elapsed_time:{elapsed_time}[sec] avg_time:{avg_time}[sec/wav]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOANWORD128_064:クェゼリン島の戦いとは，日本軍の守るクェゼリン環礁へ、アメリカ軍が侵攻して行われた戦闘である。\n",
    "# 0064: ケェヴェリンとうのたたかいとは、にっほんぐんのまもるクェヴェリンかんしょうへアメリカぐんがしんこうしたおかなわれたせんとうである。\n",
    "# キーワード登録によって「クェゼリン」を検出してみる\n",
    "\n",
    "# シーケンスに含まれるID、全てにbiasを加えてデコードしてみる\n",
    "audio = readAudioFile(f\"/mnt/e/dataset/jsut_ver1.1/loanword128/wav/LOANWORD128_064.wav\")\n",
    "input_values = feature_extractor(audio, sampling_rate=16000,return_tensors=\"pt\").input_values.to(\"cuda:0\")\n",
    "logits = model(input_values)[0]\n",
    "pred_ids = torch.argmax(logits, axis=-1)\n",
    "org_text=processor.batch_decode(pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logitsを受け取り、wordのbiasを加えたlogitsを返す関数\n",
    "def simple_bias_strategy(word,logits,bias):\n",
    "    # idに変換\n",
    "    ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
    "    logits[0,:,ids]+=bias\n",
    "    return logits\n",
    "\n",
    "logits = model(input_values)[0]\n",
    "logits = simple_bias_strategy(\"クェゼリン\",logits,5.0)\n",
    "pred_ids_with_bias = torch.argmax(logits, axis=-1)\n",
    "\n",
    "#decode結果を比較\n",
    "[org_text,processor.batch_decode(pred_ids_with_bias)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解：クェゼリン　に対して\n",
    "\n",
    "ケェヴェリン　→　クェェリン\n",
    "クェヴェリン　→　クェゼェリン\n",
    "\n",
    "になった。惜しい感じ\n",
    "\n",
    "ヴェ→ゼ\n",
    "になれば正解する雰囲気がある。\n",
    "\n",
    "もっと詳細に結果を検討する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmaxでデコードされた文字列を探し、その部分の上位N件のトークンを表示する\n",
    "def findTopKByDecodedText(needle,logits,k=5):\n",
    "    needle_ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(needle))\n",
    "    argmax_result = torch.argmax(logits, axis=-1).cpu()\n",
    "    argmax_result = argmax_result[0]\n",
    "    #argmax_resultからneedle_idsが全てマッチするかをチェック（PADであった場合無視する）、マッチしていれば先頭のindexを返す\n",
    "    start_index=-1\n",
    "    end_index=-1\n",
    "    needle_index=0\n",
    "    for i in range(len(argmax_result)-len(needle_ids)):\n",
    "        end_index = i       \n",
    "        current_id = argmax_result[i]\n",
    "        if current_id==tokenizer.pad_token_id:\n",
    "            continue\n",
    "        if current_id==needle_ids[needle_index]:\n",
    "            #print(f\"match {i} {current_id} {needle[needle_index]}\")\n",
    "            if needle_index==0:\n",
    "                start_index=i\n",
    "            needle_index+=1            \n",
    "            if needle_index==len(needle_ids):\n",
    "                break\n",
    "        elif current_id == last_id:\n",
    "            #print(f\"dup {i} {current_id} {needle[needle_index]}\")            \n",
    "            continue\n",
    "        else:\n",
    "            if needle_index>0:\n",
    "                #print(f\"unmatch {i} {current_id} {needle[needle_index]}\")\n",
    "                pass\n",
    "            start_index=-1\n",
    "            needle_index=0\n",
    "        last_id=current_id\n",
    "\n",
    "    if start_index==-1:\n",
    "        return None            \n",
    "    #start_indexからend_index+1のk件のトークンを取得\n",
    "    print(f\"start_index={start_index} end_index={end_index}\")\n",
    "    topk_result = torch.topk(logits[0,start_index:end_index+1,:],k)\n",
    "\n",
    "    result_token=[]\n",
    "    result_score=[]\n",
    "    #トークンを文字列に変換\n",
    "    for i,ids in enumerate(topk_result.indices):\n",
    "        result_token.append(tokenizer.convert_ids_to_tokens(ids.tolist()))\n",
    "        result_score.append(topk_result.values[i].tolist())\n",
    "    return [result_token,result_score]\n",
    "\n",
    "logits = model(input_values)[0]\n",
    "[\n",
    "    findTopKByDecodedText(\"ケェヴェリン\",logits,k=5),\n",
    "    findTopKByDecodedText(\"クェェリン\", simple_bias_strategy(\"クェゼリン\",logits,5.0),k=5)\n",
    "]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異なる戦略を試す。\n",
    "\n",
    "# 単純にシーケンス内の全部のIDにbiasを加えても認識結果は良くならないので、順序を考慮したbiasを加える\n",
    "# 1.全体にシーケンス内の最初のIDにbiasを加える\n",
    "# 2.最初のIDがtopに来たら、次のIDにbiasを加える\n",
    "# 3.最後のIDまで一致するまでbiasを足す\n",
    "# 4.logitsの最後までやる\n",
    "# この処理を行ったlogitsをdecodeする\n",
    "\n",
    "def sequence_bias_strategy(word,logits,bias):\n",
    "    logits = logits.clone()\n",
    "    ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
    "    #最初のidをbiasで追加する\n",
    "    logits[0,:,ids[0]]+=bias\n",
    "    id_index=0\n",
    "    time_step_length=len(logits[0])-1\n",
    "    for i in range(time_step_length):\n",
    "        max_id = torch.argmax(logits[0,i,:])\n",
    "        if max_id == ids[id_index]:\n",
    "            # 一致したら次のidに進む\n",
    "            id_index+=1\n",
    "            if id_index == len(ids):\n",
    "                # 最後のidまで一致したので最初からやり直す\n",
    "                id_index=0\n",
    "                continue\n",
    "        else:\n",
    "            #　一致しなかった場合\n",
    "            # 違う認識結果の場合と、前回の繰り返しと、PADの場合がある。\n",
    "            if max_id == tokenizer.pad_token_id:\n",
    "                #PADの場合は何もしない\n",
    "                pass\n",
    "            elif id_index > 0 and max_id == ids[id_index-1]:\n",
    "                # 前回と同じ場合はPADとidsにbiasを加える\n",
    "                logits[0,i,tokenizer.pad_token_id]+=bias\n",
    "                logits[0,i,ids[id_index]]+=bias\n",
    "                pass\n",
    "            else:\n",
    "                # 違う認識結果の場合なのか、認識しそこなっているかの判断はできない…\n",
    "                logits[0,i,ids[id_index]]+=bias\n",
    "                pass\n",
    "    return logits\n",
    "\n",
    "logits = model(input_values)[0]\n",
    "# logitsのコピー\n",
    "processor.batch_decode(torch.argmax(sequence_bias_strategy(\"クェゼリン\",logits,5.0), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シーケンスバイアスは一定の効果はあるようだが、bias設定ステージでは、違う言葉なのか、認識をミスっているのかはわからないので、どうにもならない気がする。\n",
    "\n",
    "decodeで一意に結果を求めるのではなく、キーワードのシーケンスの累積スコア（または対数確率の合算）で、キーワードが出現した可能性を数値したほうが合理的かもしれない。\n",
    "\n",
    "ひとまず、シーケンス[A,B,C]についてスコアの累積を行うことを考えてみると、\n",
    "\n",
    "1. キーワードが含まれているかもしれない、logits内のtime_stepの範囲を決める（どうやって？という話だが、総当たりでも、topKで最初と最後のIDを見つけても良い\n",
    "2. 範囲内で、Aのスコアを得る。\n",
    "3. 次のtime_stepでBのスコアを得るが、Aの繰り返し、またはPADよりも低かった場合は、次のステップへ行く\n",
    "4. BのスコアがAまたはPADよりも高い場合は、Bのスコアを得る\n",
    "5. Cのスコアに対しても同じ処理をする\n",
    "6. マッチが終わるか、logits範囲が終わったらスコアを返す（残りは無視\n",
    "\n",
    "このスコアの合算値は妥当だろうか？（logits内の別time_stepのスコアが合算できるものかはさておいて…）\n",
    "\n",
    "例えばキーワードが\n",
    "「ボボボーボ」\n",
    "であれば、3の処理があるため、\n",
    "ボーボの合算値しか出ないことがある。まあ、連続する文字の認識についてはスコアにゲタを吐かせてもいいのかしれないが…\n",
    "\n",
    "ひとまず、今は考えない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(input_values)[0]\n",
    "torch.softmax(logits[0,:,:],dim=1)\n",
    "topk_result = torch.topk(logits[0,:,:],10)\n",
    "#topk_result.indicesからPADトークンを探す\n",
    "result=torch.where(topk_result.indices==tokenizer.pad_token_id)\n",
    "18 in result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=torch.tensor([[0,1,2,3,4,5,6,7,8,9],[10,9,8,7,6,5,4,3,2,1]])\n",
    "# 8のindexを取得\n",
    "result=torch.where(test==8)\n",
    "0 in result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単純なスコア\n",
    "def get_score_simple(logits,index,id):\n",
    "    return logits[index,id]\n",
    "\n",
    "# top10の何位かでスコアを決める\n",
    "def get_score_by_rank(logits,index,id):\n",
    "    top10 = torch.topk(logits[index,:],10)\n",
    "    rank=torch.where(top10.indices == id)\n",
    "    if len(rank[0])==0:\n",
    "        return 0\n",
    "    return 10 - rank[0]\n",
    "\n",
    "# probの合計でスコアを決める\n",
    "def get_score_by_prob(logits,index,id):\n",
    "    return torch.softmax(logits[index,:],dim=0)[id]\n",
    "    \n",
    "\n",
    "def calc_score(word,sub_logits,get_score):\n",
    "    ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
    "    score=0\n",
    "    id_index=0\n",
    "    for i in range(len(sub_logits)):\n",
    "        target_score=get_score(sub_logits,i,ids[id_index])\n",
    "        if id_index>0:\n",
    "            last_id_score=get_score(sub_logits,i,ids[id_index-1])\n",
    "        else:\n",
    "            last_id_score=0\n",
    "        pad_score=get_score(sub_logits,i,tokenizer.pad_token_id)\n",
    "        if target_score>last_id_score and target_score>pad_score:\n",
    "            score+=target_score\n",
    "            id_index+=1\n",
    "            if id_index==len(ids):\n",
    "                break\n",
    "    return score\n",
    "\n",
    "logits = model(input_values)[0]\n",
    "\n",
    "[\n",
    "[\n",
    "    calc_score(\"クェゼリン\",logits[0,18:37,:],get_score_simple),\n",
    "    calc_score(\"ケェヴェリン\",logits[0,18:37,:],get_score_simple),\n",
    "    calc_score(\"テスト\",logits[0,18:37,:],get_score_simple)\n",
    "],\n",
    "[\n",
    "    calc_score(\"クェゼリン\",logits[0,18:37,:],get_score_by_rank),\n",
    "    calc_score(\"ケェヴェリン\",logits[0,18:37,:],get_score_by_rank),\n",
    "    calc_score(\"テスト\",logits[0,18:37,:],get_score_by_rank)\n",
    "],\n",
    "[\n",
    "    calc_score(\"クェゼリン\",logits[0,18:37,:],get_score_by_prob),\n",
    "    calc_score(\"ケェヴェリン\",logits[0,18:37,:],get_score_by_prob),\n",
    "    calc_score(\"テスト\",logits[0,18:37,:],get_score_by_prob)\n",
    "]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 普通にscoreの合算で差異は出るので、後は、キーワード毎の比較ができるように正規化すれば良い\n",
    "# 今のやり方だと長いシーケンスはそれだけスコアが高くなるので、シーケンスの長さで割る\n",
    "def calc_score_normalized(word,sub_logits,get_score):\n",
    "    return calc_score(word,sub_logits,get_score) / len(tokenizer.tokenize(word))\n",
    "\n",
    "[\n",
    "[\n",
    "    calc_score_normalized(\"クェゼリン\",logits[0,18:37,:],get_score_simple),\n",
    "    calc_score_normalized(\"ケェヴェリン\",logits[0,18:37,:],get_score_simple),\n",
    "    calc_score_normalized(\"テスト\",logits[0,18:37,:],get_score_simple)\n",
    "],\n",
    "[\n",
    "    calc_score_normalized(\"クェゼリン\",logits[0,18:37,:],get_score_by_rank),\n",
    "    calc_score_normalized(\"ケェヴェリン\",logits[0,18:37,:],get_score_by_rank),\n",
    "    calc_score_normalized(\"テスト\",logits[0,18:37,:],get_score_by_rank)\n",
    "],\n",
    "[\n",
    "    calc_score_normalized(\"クェゼリン\",logits[0,18:37,:],get_score_by_prob),\n",
    "    calc_score_normalized(\"ケェヴェリン\",logits[0,18:37,:],get_score_by_prob),\n",
    "    calc_score_normalized(\"テスト\",logits[0,18:37,:],get_score_by_prob)\n",
    "]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まとめ\n",
    "\n",
    "1. logitsのtopK(10)を取る（time_step*10のtensorになる）\n",
    "2. それぞれのtime_stepのsoftmaxを取る\n",
    "3. 検出キーワードの最初の文字から最後の文字までのprobを足し合わせるわけだが、どこからキーワードが始まっているかはまだ確定できない\n",
    "4. 最初のトークンが始まるtime_stepを探す（＝topK内にトークンが現れる）あったら、そこのprobをスコアに足す\n",
    "5. 次のtime_stepで次のトークンが存在したらprobを足して、次のトークンへ\n",
    "6. 存在していなかった場合、前回の繰り返し or PADであれば、次のtimestepへ\n",
    "7. どれも違う場合は、マッチ失敗とみなしスコアを放棄し、4へ\n",
    "8. シーケンスの最後に到達した場合は、スコアをキーワードのスコアとして、appendする（複数検出するかもしれないので）\n",
    "9. time_stepの最後に到達した場合は終了\n",
    "\n",
    "基本的に、O(n) n = time_step 処理量になるので、キーワードが複数あるO(n^2)になる。まあまあ重いがtopKで刈り込んでるからいい？（複数キーワードは並列処理できるけど…）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 209, 'end': 224, 'prob': 0.96728515625}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_keyword_avgprobs(word,logits):\n",
    "    ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
    "    logits_prob=torch.softmax(logits[0,:,:],dim=1)\n",
    "    avg_probs=[]\n",
    "    start_index=0\n",
    "    id_index=0\n",
    "    current_prob_total=0.0\n",
    "    unmatch_count=0\n",
    "    for i in range(len(logits[0])):\n",
    "        pad_prob=logits_prob[i,tokenizer.pad_token_id]\n",
    "        if id_index>0:\n",
    "            last_id_prob=logits_prob[i,ids[id_index-1]]\n",
    "        else:\n",
    "            start_index=i\n",
    "            last_id_prob=0.0\n",
    "        current_prob=logits_prob[i,ids[id_index]]\n",
    "        # 0.1以上の確率がある場合はマッチ、最初ではない場合は0.01でもよい\n",
    "        if current_prob > 0.1 or (id_index>0 and current_prob > 0.01):\n",
    "            #print(f\"found {i} {tokenizer.convert_ids_to_tokens(ids[id_index])}({ids[id_index]}) top1 is {tokenizer.convert_ids_to_tokens(top1_id)}({top1_id}) current={current_prob} last={last_id_prob} pad={pad_prob} (total={current_prob_total})\")\n",
    "            current_prob_total+=current_prob\n",
    "            #次へ\n",
    "            id_index+=1\n",
    "            if id_index==len(ids):\n",
    "                #終わったので平均を計算して格納\n",
    "                avg_probs.append({\n",
    "                    \"start\": start_index,\n",
    "                    \"end\": i,\n",
    "                    \"prob\": (current_prob_total / len(ids)).item()\n",
    "                })\n",
    "                id_index=0\n",
    "                current_prob_total=0.0\n",
    "                unmatch_count=0                \n",
    "        elif last_id_prob > 0.1 or pad_prob > 0.1:\n",
    "            # PADや前回のIDの確率が0.1以上の場合は、何もしない\n",
    "            pass\n",
    "        else:\n",
    "            #それ以外の場合は、マッチが失敗したとみなす\n",
    "            #print(f\"unmatch {i} expect {tokenizer.convert_ids_to_tokens(ids[id_index])}({ids[id_index]}) top1 is {tokenizer.convert_ids_to_tokens(top1_id)}({top1_id}) current={current_prob} last={last_id_prob} pad={pad_prob} \")\n",
    "            if unmatch_count > 0:\n",
    "                #print(f\"unmatch count is {unmatch_count} so reset\")\n",
    "                id_index=0\n",
    "                current_prob_total=0.0\n",
    "                unmatch_count=0\n",
    "            else:\n",
    "                unmatch_count+=1\n",
    "    return avg_probs\n",
    "\n",
    "logits = model(input_values)[0]\n",
    "get_keyword_avgprobs(\"アメリカ\",logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time:11.659764051437378[sec] avg_time:0.11659764051437378[sec/keyword]\n"
     ]
    }
   ],
   "source": [
    "# benchmark\n",
    "import time\n",
    "logits = model(input_values)[0]\n",
    "start_time = time.time()\n",
    "for i in range(100):\n",
    "    get_keyword_avgprobs(\"クェゼリン\",logits)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"elapsed_time:{elapsed_time}[sec] avg_time:{elapsed_time/100}[sec/keyword]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ケェヴェリンとうのたたかいとは、にっほんぐんのまもるクェヴェリンかんしょうへアメリカぐんがしんこうしたおかなわれたせんとうである。']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'アメリカ', 'start': 209, 'end': 224, 'prob': 0.96728515625},\n",
       " {'word': 'にっぽん', 'start': 99, 'end': 112, 'prob': 0.720703125},\n",
       " {'word': 'クェゼリン', 'start': 152, 'end': 172, 'prob': 0.6806640625},\n",
       " {'word': 'まもる', 'start': 46, 'end': 144, 'prob': 0.345703125}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同一区間は与えられたキーワードのどれかであるか、どれでもないか、なので、複数キーワードを一括で処理できるようにする\n",
    "\n",
    "class Beam:\n",
    "    def __init__(self,word,priority,result):\n",
    "        self.id=word\n",
    "        self.priority=priority\n",
    "        self.result=result\n",
    "        self.ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
    "        self.start_index=0\n",
    "        self.prob_total=0.0\n",
    "        self.unmatch_count=0\n",
    "        self.id_index=0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.id} {self.result}\"\n",
    "    \n",
    "    def reset(self):\n",
    "        self.start_index=0\n",
    "        self.prob_total=0.0\n",
    "        self.unmatch_count=0\n",
    "        self.id_index=0\n",
    "\n",
    "    # 不一致の時のハンドラ。余計な発音が挟まってる場合があるので、二回連続まで許容する\n",
    "    def unmatch(self):\n",
    "        #print(f\"unmatch {i} expect {tokenizer.convert_ids_to_tokens(ids[id_index])}({ids[id_index]}) top1 is {tokenizer.convert_ids_to_tokens(top1_id)}({top1_id}) current={current_prob} last={last_id_prob} pad={pad_prob} \")\n",
    "        if self.unmatch_count > 0:\n",
    "            #print(f\"unmatch count is {unmatch_count} so reset\")\n",
    "            self.reset()\n",
    "            return True\n",
    "        else:\n",
    "            self.unmatch_count+=1\n",
    "            return False\n",
    "            \n",
    "    def step(self,probs,step,threshold):        \n",
    "        current_id=self.ids[self.id_index]\n",
    "        step_prob=probs[current_id]\n",
    "\n",
    "        # id_index==0の時は、BEAM段階で刈り取られているので、調べるまでもなく、thresholdを上回っている（筈）\n",
    "        if self.id_index == 0:\n",
    "            self.start_index=step\n",
    "        elif step_prob < threshold:\n",
    "            return self.unmatch()\n",
    "\n",
    "        self.id_index+=1\n",
    "        self.unmatch_count=0\n",
    "        self.prob_total+=step_prob\n",
    "        if self.id_index == len(self.ids):\n",
    "            #終了\n",
    "            # 0.01以下の場合は、結果としては切り捨てる\n",
    "            if self.current_prob() > 0.01:\n",
    "                self.result.append({\n",
    "                    \"word\": self.id,\n",
    "                    \"start\": self.start_index,\n",
    "                    \"end\": step,\n",
    "                    \"prob\": (self.prob_total / len(self.ids)).item()\n",
    "                })\n",
    "            self.reset()\n",
    "            return\n",
    "\n",
    "    def current_prob(self):\n",
    "        if self.id_index > 0:\n",
    "            return self.prob_total / self.id_index\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def max_prob(self):\n",
    "        return max(self.result,key=lambda x:x[\"prob\"])\n",
    "\n",
    "def get_keywords_avgprobs(words,logits,num_beams=5,without_pad=True):\n",
    "    beams={}\n",
    "    result=[]\n",
    "    for i,word in enumerate(words):\n",
    "        beam = Beam(word,i,result)\n",
    "        first_token=beam.ids[0]\n",
    "        if beams.get(first_token) is None:\n",
    "            beams[first_token]=[]\n",
    "        beams[first_token].append(beam)\n",
    "\n",
    "    logits_prob=torch.softmax(logits[0,:,:],dim=1, dtype=torch.float16)\n",
    "    # 上位N件の確率を取得\n",
    "    topk_values, topk_indices = torch.topk(logits_prob, num_beams) #num_beamsにしているけど、必然性はない\n",
    "    threshold = topk_values[:,-1]\n",
    "\n",
    "    # padが一位のstepを除外しておく（10倍以上高速化するが認識率は悪化する) \n",
    "    if without_pad:\n",
    "        steps=torch.where(topk_indices[:,0]!=tokenizer.pad_token_id)[0].tolist()\n",
    "    else:\n",
    "        steps=range(len(logits_prob))\n",
    "\n",
    "    current_beams=[]\n",
    "    for i in steps:\n",
    "        topKList=topk_indices[i,:].tolist()\n",
    "\n",
    "        # topKにあるものを候補に加える。これはnum_beamsとは別枠\n",
    "        for id in topKList:\n",
    "            current_beams.extend(beams.get(id,[]))\n",
    "\n",
    "        #重複を排除\n",
    "        current_beams=list(set(current_beams))\n",
    "        \n",
    "        #print(f\"step={i} beam={len(current_beams)} {list(map(lambda x:x.id,current_beams))} {tokenizer.convert_ids_to_tokens(topKList)}\")\n",
    "\n",
    "        next_beams=[]\n",
    "        for beam in current_beams:\n",
    "            beam.step(logits_prob[i,:],i,threshold[i])\n",
    "            if beam.id_index != 0:\n",
    "                #マッチしたものだけ次も継続する\n",
    "                next_beams.append(beam)\n",
    "\n",
    "        if len(next_beams) > num_beams:\n",
    "            #確率の高いものからnum_beams件だけ残す\n",
    "            current_beams=sorted(next_beams,key=lambda x: -x.current_prob())[:num_beams]\n",
    "        else:\n",
    "            current_beams=next_beams\n",
    "\n",
    "    return sorted(result,key=lambda x: -x[\"prob\"])\n",
    "\n",
    "keywords=[\n",
    "        # 適当なユニーク文字列（かな、カタカナ）\n",
    "        \"アメリカ\",\"にっぽん\",\"まもる\",\"クェゼリン\",\"テスト\",\n",
    "        \"あした\",\"あさって\",\"はい\",\"いいえ\",\"イエス\",\"ノー\",\n",
    "        \"ホグワーツ\",\"ハリーポッター\",\"ハーマイオニー\",\"ロン\",\"ダンブルドア\",\n",
    "        \"まどか\",\"さやか\",\"ほむら\",\"マミ\",\"キュゥべえ\",\n",
    "        \"あおい\",\"あかね\",\"さくら\",\"みどり\",\"きいろ\",\n",
    "        \"ハンバーグ\",\"ステーキ\",\"ミートソース\",\"ミートボール\",\"ハム\",\n",
    "        \"せかい\",\"ちきゅう\",\"うみ\",\"そら\",\"ほし\",\n",
    "        \"くるま\",\"でんしゃ\",\"ひこうき\",\"ふね\",\"じてんしゃ\",\n",
    "        \"ねこ\",\"いぬ\",\"うさぎ\",\"ねずみ\",\"とら\",\n",
    "        \"にんげん\",\"おとこ\",\"おんな\",\"こども\",\"おじいさん\",\n",
    "        \"ブッシュ\",\"トランプ\",\"バイデン\",\"ヒラリー\",\"オバマ\"\n",
    "]\n",
    "\n",
    "logits = model(input_values)[0]\n",
    "print(org_text)\n",
    "get_keywords_avgprobs(keywords,logits,num_beams=5,without_pad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time:1.937535047531128[sec] avg_time:0.06458450158437093[sec/iteration] avg_time:0.0011532946711494808[sec/keyword]\n"
     ]
    }
   ],
   "source": [
    "# benchmark\n",
    "import time\n",
    "logits = model(input_values)[0]\n",
    "start_time = time.time()\n",
    "bench_count=30\n",
    "for i in range(bench_count):\n",
    "    get_keywords_avgprobs(keywords,logits,num_beams=5,without_pad=True)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"elapsed_time:{elapsed_time}[sec] avg_time:{elapsed_time/bench_count}[sec/iteration] avg_time:{elapsed_time/bench_count/len(keywords)}[sec/keyword]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
